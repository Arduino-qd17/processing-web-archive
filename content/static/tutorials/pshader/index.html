<h1>Shaders in Processing</h1>
	
<p>
<table width="656">
   	<tr>
   	
	<p><em>This tutorial is for Processing version 2.0+. If you see any errors or have comments, please <a href="https://github.com/processing/processing-web/issues?state=open">let us know</a>.</em></p>
   	   		
	<p>The source code contained in this tutorial is also available at <a href="https://github.com/codeanticode/pshader-tutorials">https://github.com/codeanticode/pshader-tutorials</a></p>
	
	<p>&nbsp;</p>

<h3>1. What is a (P)shader?</h3>

<p>A brand new feature in Processing 2.0 is the inclusion of GLSL shaders. In fact, everything that Processing draws on the screen with the <a href="http://processing.org/reference/size_.html">P2D and P3D renderers</a> is the output of an appropriate "default shader" running behind the scenes. Processing handles these default shaders transparently so that the user doesn't need to worry about them, and she or he can continue to use the well-known <a href="http://processing.org/reference/">drawing API</a> and expect the same visual results as with  previous versions of Processing. However, Processing 2.0 incorporates a new set of API functions and variables that allows advanced users to replace the default shaders with her or his own. This opens-up many exciting possibilities: rendering 3D scenes using more sophisticated lighting and texturing algorithms, applying image post-processing effects in real-time, creating complex procedural objects that would be very hard or impossible to generate with other techniques, and sharing shader effects between desktop, mobile and web platforms with minimal code changes.</p>

<p>In order to understand how this new shader API works and how can be used to extend the drawing capabilities of Processing, it is necessary to have an overview of the key concepts of shader programming, first in general and then from the "point of view" of a Processing sketch (so get ready and grab a beverage of your preference, because this is going to be a very long tutorial).</p>

<p>Answering the question alluded by the title of this section: a shader is basically a program that runs on the Graphics Processing Unit (GPU) of the computer, and generates the visual output we see on the screen given the information that defines a 2D or 3D scene: vertices, colors, textures, lights, etc. The term "shader" itself might be slightly misleading, since the word shading in the context of drawing implies the process of representing different levels of darkness on the surface of an object due to the surrounding lights in order to create the illusion of depth. The <a href="http://renderman.pixar.com/view/brief-introduction-to-renderman">first computer shaders</a> were mainly concerned with the synthetic calculation of these shading levels given the mathematical representation of a tridimensional scene and the material properties of the objects in it, and attempted to create photorealistic renderings of such scenes. Nowadays, the shaders are not only used to calculate the shading or lighting levels in a virtual scene, but they are responsible of all the rendering stages, starting with camera transformations that are applied on the raw geometry, and ending at the evaluation of the final color of each visible pixel in the screen.
</p>

<p>There are several languages that can be used to write shaders, such as <a href="https://developer.nvidia.com/cg-toolkit">Cg</a>, <a href="http://msdn.microsoft.com/en-us/library/windows/desktop/bb509561(v=vs.85).aspx">HLSL</a> and <a href="http://www.opengl.org/documentation/glsl/">GLSL</a>. The latter is the shader language included in <a href="http://www.opengl.org/">OpenGL</a>, the standard rendering library and API used across a wide variety of computing devices, ranging from high-end desktop computers to smartphones. GLSL simply stands for OpenGL Shading Language. Since Processing uses OpenGL as the basis for its P2D and P3D renderers, GLSL is the shader language that one has to use to write custom shaders to include in Processing sketches.</p>

<p>Writing shaders requires an understanding of the individual stages involved in the rendering a scene with the GPU, and how we can use GLSL to program them. The sequence of these stages is called the "graphical pipeline" in the technical parlance of Computer Graphics, and we will now take a look at the main stages in the pipeline from the perspective of a Processing sketch.</p>

<p>Note that the goal of this document is not to provide a programming guide to GLSL, but to describe in detail the new shader API in Processing so that users already familiar with GLSL can write their own custom shaders and then use them in Processing. There are several resources, such as <a href="http://www.lighthouse3d.com/tutorials/">online</a> <a href="http://ogldev.atspace.co.uk/">tutorials</a> and <a href="http://www.opengl.org/discussion_boards/forum.php">forums</a>, <a href="http://www.amazon.com/OpenGL-Shading-Language-3rd-Edition/dp/0321637631/ref=sr_1_1?">books</a>, and <a href="http://glsl.heroku.com/">coding sandboxes</a>, that can be recommended for learning GLSL programming. Furthermore, the GLSL experience gained using a different programming interface, platform or toolkit (openFrameworks, cinder, webGL, iOS, Android, etc.) can be easily translated over to Processing.
Let's start with a simple 3D sketch as the model to understand the relationship between the Processing functions and variables and the underlying pipeline running on the GPU. This sketch draws a quad with lights and some geometric transformations applied to it:

<table>
<caption><b>Code listing 1.1:</b> Simple 3D sketch that draws a lit rotating rectangle</caption>
<tr><td>
<pre>
<span style="color: #CC6633;">float</span> angle;
<span style="color: #996633;">void</span> <span style="color: #006699;"><b>setup</b></span>() {
&nbsp;&nbsp;<span style="color: #006699;">size</span>(400, 400, <span style="color: #666666;">P3D</span>);
&nbsp;&nbsp;<span style="color: #006699;">noStroke</span>();  
}

<span style="color: #996633;">void</span> <span style="color: #006699;"><b>draw</b></span>() {
&nbsp;&nbsp;<span style="color: #006699;">background</span>(0);
&nbsp;&nbsp;
&nbsp;&nbsp;<span style="color: #006699;">camera</span>(<span style="color: #FF6699;">width</span>/2, <span style="color: #FF6699;">height</span>/2, 300, <span style="color: #FF6699;">width</span>/2, <span style="color: #FF6699;">height</span>/2, 0, 0, 1, 0);
&nbsp;&nbsp;<span style="color: #006699;">pointLight</span>(200, 200, 200, <span style="color: #FF6699;">width</span>/2, <span style="color: #FF6699;">height</span>/2, 200);
&nbsp;&nbsp;
&nbsp;&nbsp;<span style="color: #006699;">translate</span>(<span style="color: #FF6699;">width</span>/2, <span style="color: #FF6699;">height</span>/2);
&nbsp;&nbsp;<span style="color: #006699;">rotateY</span>(angle);
&nbsp;&nbsp;
&nbsp;&nbsp;<span style="color: #006699;">beginShape</span>(<span style="color: #666666;">QUADS</span>);
&nbsp;&nbsp;<span style="color: #006699;">normal</span>(0, 0, 1);
&nbsp;&nbsp;<span style="color: #006699;">fill</span>(50, 50, 200);
&nbsp;&nbsp;<span style="color: #006699;">vertex</span>(-100, +100);
&nbsp;&nbsp;<span style="color: #006699;">vertex</span>(+100, +100);
&nbsp;&nbsp;<span style="color: #006699;">fill</span>(200, 50, 50);
&nbsp;&nbsp;<span style="color: #006699;">vertex</span>(+100, -100);
&nbsp;&nbsp;<span style="color: #006699;">vertex</span>(-100, -100);
&nbsp;&nbsp;<span style="color: #006699;">endShape</span>();  
&nbsp;&nbsp;
&nbsp;&nbsp;angle&nbsp;+=&nbsp;0.01;
}
</pre>
</td></tr>
</table>

<p>
The image below depicts a diagram of the graphics pipeline, and how the input of the pipeline is related to the function calls in the sketch. There are several additional stages in a typical pipeline, but we don't show them here in the sake of clarity. Furthermore, Processing follows the specification set by <a href="http://www.khronos.org/opengles/">OpenGL ES</a>, which is the version of OpenGL used in mobile devices and, through <a href="<a href="http://www.khronos.org/opengles/">">WebGL</a>, also in browsers. The programming model in OpenGL ES is simpler and doesn't include all the stages that are present in desktop OpenGL. On the desktop, OpenGL ES is in fact subset of OpenGL, so this choice ensures that GLSL shaders written to work with Processing can be used across different platforms with minimal changes. As a downside, advanced features of OpenGL desktop are not (directly) accessible through the Processing API, however there are several other toolkits that can be used for more sophisticated graphics programming.
</p>

<p><img src="imgs/pipeline.png"></p>

<p>
The camera, light, transformation and vertex data defined in the sketch result in two types of input for the pipeline: uniform and attribute variables. Uniform variables are those that remain constant for each vertex in the scene. The projection and modelview matrices computed from the camera and transformation setup fall in this category, since each vertex in the scene is affected by the same projection/modelview matrices. The lighting parameters (source position, color, etc) are also passed to the pipeline as uniform variables for the same reason. On the other hand, the variables that change from vertex to vertex are called attributes, and in this example we have three different type of attributes per vertex: the xyz position itself, set with the <em>vertex()</em> function, the color specified with <em>fill()</em> and the normal vector. Note that even though there is only one call to the <em>normal()</em> function, each vertex will have its own normal vector, which in this case will be the same across the four vertices.
</p>

<p>
The first stage in the pipeline is the vertex shader. It uses the vertex attributes (in this case positions, colors and normals), projection and modelview matrices, and light parameters in order to compute, for each vertex, what its position and color on the screen turns out to be. We can think of the vertex shader operating on one vertex at the time, and carrying out all the mathematical operations to project the vertex on the screen plane and to determine its color given the particular projection/modelview matrices and the arrangement of light sources.
</p>

<p>
The vertex shader doesn't know how the vertices are connected to each other forming a shape since it receives each vertex independently of the others. Therefore, the immediate output of the vertex stage is just the list of vertices projected onto the screen plane. In Processing we set how the vertices of a shape are connected with each other by passing an argument to <em>beginShape()</em>. This argument, which in the case of this example is <em>QUADS</em>, determines how the next stage in the pipeline, called primitive assembly, builds geometric primitives out of the individual vertices that come out of the vertex shader. 
</p>

<p>
Once the primitives are determined, the next stage consists in calculating which pixels in the screen are covered by the faces of the primitives being drawn. But one problem is that the screen is a discrete grid of pixels, while the geometric data up to this point is represented as continuous numeric values. The process called "rasterization" is in charge of discretizing the vertex coordinates so they can be accurately represented on the screen at the given resolution. Another problem is that the output color has been calculated only at the input vertices so far, and it needs to be determined at the rest of the pixels that lie inside the primitives. This is solved by interpolation: the color at the center of triangle is interpolated from the colors of the three corner vertices. There are several ways of carrying out this interpolation, and the distortion introduced by the perspective <a href="http://en.wikibooks.org/wiki/GLSL_Programming/Rasterization">needs to be accounted for</a> as well.
<p>

<p>
The output of the rasterization and interpolation stage are the pixel positions (x,y), together with their color (and optionally other variables that can be defined in the shader, we will cover this a few paragraphs later). This information (position, color, and other per-pixel variables) is called a fragment. The fragments are processed in the next stage, called the fragment shader. In this particular example, the fragment shader doesn't  do much, it only writes the color to the screen position (x, y). At this point it is useful to think, in the same way we think of the vertex shader as operating on each input vertex at the time, of the fragment shader as operating on each fragment coming down the pipeline at the time, and then outputting the color of the fragment to the screen. As a way to see this more clearly, we could imagine the vertex shader as a "function" that is called inside the loop that runs over all the input vertices. We could write in pseudo-code:
</p>

<pre>
for (int i = 0; i < vertexCount; i++) {
  output = vertexShader(vertex[i]);
}
</pre>

<p>where the <em>vertexShader</em> function is defined as:</p>

<pre>
function vertexShader(vertex) {
  projPos = projection * modelview * vertex.position;
  litColor = lightColor * dot(vertex.normal, lightDirection);    
  return (projPos, litColor);
}
</pre>

<p>
While for the fragment shader, we would have a loop over the visible, interpolated fragments:
</p>

<pre>
for (int i = 0; i < fragmentCount; i++) {
  screenBuffer[fragment[i].xy] = fragmentShader(fragment[i]);
}

function fragmentShader(fragment) {
  return fragment.litColor;
}
</pre>

<p>
Note that the <em>litColor</em> variable is calculated in the vertex shader, and then accessed in the fragment shader. These kind of variables that are exchanged between the vertex and the fragment shaders are called "varying". As we discussed before, the values of the varying variables are interpolated (perspective-corrected) over the fragments spanned by the vertices by the GPU hardware, so we don't need to worry about that. We can define additional varying variables, depending on the type of effect we are trying to achieve with our shaders.
</p>

<p>
The fragment shader can perform additional operations on each fragment, and in fact these could be potentially very complex. It is possible to implement a ray-tracer entirely in the fragment shader, and we will touch on this topic briefly with some more advanced examples. One important limitation of the fragment shader to keep in mind is that it cannot access fragments other than the one is currently processing (same as the vertex shader cannot access a vertex other than the one is currently operating on).
</p>

<p>
From all the stages described in our simplified pipeline, only the vertex and fragment shaders can be modified to run our own custom code. The other stages are all hard-coded in the GPU. With this picture in mind, we can move forward and start working through the actual shader API in Processing. 
</p>

<h3>2. The PShader class</h3>

<p>
In the previous section we saw that the two programmable stages in the GPU pipeline are the vertex and fragment shaders (in recent versions of OpenGL desktop there are additional programmable stages, but they are not covered by the shader API in Processing). Both are needed in order to specify a complete, working pipeline. In Processing we have to write the GLSL code for the fragment and vertex shaders in separate files, which then are combined to form a single "shader program" than can be executed in the GPU. The word "program" is often omitted, with the implicit assumption that when we just say shader we are referring to a complete shader program involving a fragment and vertex shaders.
<p>

<p>
A shader (program) is encapsulated by the <em>PShader</em> class. A <em>PShader</em> object is created with the <em>loadShader()</em> function which takes the filenames of the vertex and fragment files as the arguments. If only one filename is specified, then Processing will assume that the filename corresponds to the fragment shader, and will use a default vertex shader. Code listing 2 shows a sketch loading and using a shader that renders lights using discrete shading levels.
</p>

<table>
<caption><b>Code listing 2.1:</b> Sketch that uses a toon effect shader to render a sphere.</caption>
<tr><td>
<pre>
<span style="color: #CC6633;">PShader</span> toon;

<span style="color: #996633;">void</span> <span style="color: #006699;"><b>setup</b></span>() {
&nbsp;&nbsp;<span style="color: #006699;">size</span>(640, 360, <span style="color: #666666;">P3D</span>);
&nbsp;&nbsp;<span style="color: #006699;">noStroke</span>();
&nbsp;&nbsp;<span style="color: #006699;">fill</span>(204);
&nbsp;&nbsp;toon&nbsp;=&nbsp;<span style="color: #006699;">loadShader</span>(<span style="color: #7D4793;">&quot;ToonFrag.glsl&quot;</span>, <span style="color: #7D4793;">&quot;ToonVert.glsl&quot;</span>);
&nbsp;&nbsp;toon.<span style="color: #006699;">set</span>(<span style="color: #7D4793;">&quot;fraction&quot;</span>, 1.0);
}

<span style="color: #996633;">void</span> <span style="color: #006699;"><b>draw</b></span>() {
&nbsp;&nbsp;<span style="color: #006699;">shader</span>(toon);
&nbsp;&nbsp;<span style="color: #006699;">background</span>(0); 
&nbsp;&nbsp;<span style="color: #CC6633;">float</span> dirY = (<span style="color: #FF6699;">mouseY</span> / float(<span style="color: #FF6699;">height</span>) - 0.5) * 2;
&nbsp;&nbsp;<span style="color: #CC6633;">float</span> dirX = (<span style="color: #FF6699;">mouseX</span> / float(<span style="color: #FF6699;">width</span>) - 0.5) * 2;
&nbsp;&nbsp;<span style="color: #006699;">directionalLight</span>(204, 204, 204, -dirX, -dirY, -1);
&nbsp;&nbsp;<span style="color: #006699;">translate</span>(<span style="color: #FF6699;">width</span>/2, <span style="color: #FF6699;">height</span>/2);
&nbsp;&nbsp;<span style="color: #006699;">sphere</span>(120);
}

<b>ToonVert.glsl:</b>

uniform mat4 transform;
uniform mat3 normalMatrix;
uniform vec3 lightNormal;

attribute vec4 vertex;
attribute vec4 color;
attribute vec3 normal;

varying vec4 vertColor;
varying vec3 vertNormal;
varying vec3 vertLightDir;

void main() {
  gl_Position = transform * vertex;  
  vertColor = color;
  vertNormal = normalize(normalMatrix * normal);
  vertLightDir = -lightNormal;
}

<b>ToonFrag.glsl:</b>

#ifdef GL_ES
precision mediump float;
precision mediump int;
#endif

#define PROCESSING_LIGHT_SHADER

uniform float fraction;

varying vec4 vertColor;
varying vec3 vertNormal;
varying vec3 vertLightDir;

void main() {  
  float intensity;
  vec4 color;
  intensity = max(0.0, dot(vertLightDir, vertNormal));

  if (intensity > pow(0.95, fraction)) {
    color = vec4(vec3(1.0), 1.0);
  } else if (intensity > pow(0.5, fraction)) {
    color = vec4(vec3(0.6), 1.0);
  } else if (intensity > pow(0.25, fraction)) {
    color = vec4(vec3(0.4), 1.0);
  } else {
    color = vec4(vec3(0.2), 1.0);
  }

  gl_FragColor = color * vertColor;  
}
</pre>
</td></tr>
</table>

<p>
The ifdef section in the fragment shader is required to make the shader compatible with OpenGL ES and WebGL. It sets the precision of the float and integer numbers to medium, which should be fine for most devices. These precision statements are optional on the desktop. There is also a #define line declaring the PROCESSING_LIGHT_SHADER symbol. This define is used by Processing to determine the type of shader, and whether or not it is valid to render the geometry in the sketch. This topic will be explain in detail in the next section. 
</p>

<p>
There are three varying variables shared between the fragment and vertex shaders, vertColor, vertNormal and vertLightDir, which are used to do the lighting calculation per-pixel, instead of per-vertex, which is the default. We will see an detailed example about per-pixel lighting later on in this tutorial. The gl_Position variable in the vertex shader is a GLSL built-in variable used to store the output vertex position, while gl_FragColor is the corresponding built-in variable to store the color output for each fragment. The value of gl_FragColor continues down the pipeline through the (non-programmable) stages of alpha blending and depth occlusion in order to compute the final color on the screen.
</p>

<p>
The PShader class includes the set() function to pass values from the sketch to the uniform variables in the fragment or vertex shaders. As discussed earlier, the uniform variables remain constant for each vertex in the scene, so they need to be set only once for the whole shape or shapes being rendered. Some uniform variables are automatically set by Processing, such as transform, normalMatrix and lightNormal. The next section describes in detail the names of uniform and attribute variables that are automatically set by the Processing renderers.
</p>


<h3>3. Types of shaders in Processing</h3>

<p>
From the code and pseudo-code examples presented in the previous sections, it can be appreciated that different shaders have different uniform and attribute variables. Clearly, a shader that does not compute lighting does not need uniforms holding the light source's position and color, for example. If we work with a low-level toolkit in C or C++ with direct access to the OpenGL API, we are free to name the uniforms and attributes of a shader in any way we like, since we have absolute control on the way the geometry is stored in our application, and how and when it is passed over to the GPU pipeline using the OpenGL functions. This is different when working in Processing, since the shaders are handled automatically by the renderers and should be able to handle the geometry that is described with the drawing API of Processing. This doesn't imply that custom shaders must render things in the same way as Processing does by default, quite in the contrary, the use of custom shaders opens up the possibility of greatly altering the rendering pipeline in Processing. However, custom shaders meant to be used in conjunction with the standard drawing API have to follow certain naming conventions, and are bound by some limitations. 
</p>

<p>
Depending on whether a scene has strokes (lines or points), or it uses textures and/or lights, then the shader to be used under those circumstances must present a specific set of uniform and attribute variables that will allow Processing to interface with the shader and send the appropriate data to it.
</p>

<p>
Based on this uniform and attribute requirements, shaders in Processing must belong to one of 6 different types. These 6 types can be grouped in 3 classes:
</p>

<ul>
<li>POINT shaders: used to render stroke points.</li> 
<li>LINE shaders: used to render stroke lines.</li> 
<li>TRIANGLE shaders: used to render anything else, which means that they will handle (lit/unlit, textured/non-textured) shapes. Because all shapes in Processing are ultimately made out of triangles, they are can be called TRIANGLE shaders.</li> 
</ul>

<p>
When setting a shader with the <em>shader()</em> function, if no type argument is specified then the shader will be assumed to be a triangle shader:
</p>

<pre>
<span style="color: #006699;"><b>draw</b></span>() {
&nbsp;&nbsp;<span style="color: #006699;">shader</span>(pointShader, <span style="color: #666666;">POINTS</span>);
&nbsp;&nbsp;<span style="color: #006699;">shader</span>(lineShader, <span style="color: #666666;">LINES</span>);
&nbsp;&nbsp;<span style="color: #006699;">shader</span>(polyShader); <span style="color: #7E7E7E;">// same as shader(polyShader, TRIANGLE);</span>

&nbsp;&nbsp;<span style="color: #006699;">stroke</span>(255);
&nbsp;&nbsp;<span style="color: #006699;">beginShape</span>(POLYGON);
&nbsp;&nbsp;...
}
</pre>

<p>
This code will result in Processing using <em>pointShader</em> to render stroke points, <em>lineShader</em> to render lines, and <em>polyShader</em> to render triangles (anything else). 
</p>

<p>
The point and line shaders are relevant only when using the P3D renderer. The rendering of points and lines in three dimensions is very different from that of regular geometry because they need to be always facing the screen. This requires a different transformation math in the vertex shader. By contrast, all geometry in two dimensions, including lines and points, are rendered as regular triangles since it is contained in a single plane parallel to the screen.
</p>

<p>
However, there are different types of triangle shaders because of the use of lights and textures. There are in fact 4 different situations when rendering regular, non-stroked geometry:
</p>

<ul>
<li>there are no lights and no textures</li>
<li>there are lights but no textures</li>
<li>there are textures but no lights</li>
<li>there are both textures and lights</li>
</ul>

<p>
As mentioned earlier, a shader for rendering lit geometry requires additional attributes and uniform variables to be sent from Processing that are not needed by a shader that only renders flat-colored geometry without lights or textures. Similarly, a shader for rendering textured polygons needs its own uniforms and attributes (texture sampler and texture coordinates) that would not be required otherwise. Although it could have been possible to have a generic shader in Processing that takes care of all different rendering scenarios, this would be very inefficient because of all the branching that should take place inside the shaders to choose the appropriate rendering path. In general, GPUs are not very efficient at handling branches in their shaders, particularly those available on mobile devices.
</p>

<p>In consequence, the triangle shaders are further discriminated in 4 types:</p>

<ul>
<li>COLOR: render geometry with no lights and no textures</li>
<li>LIGHT: render geometry with lights but no textures</li>
<li>TEXTURE: render geometry with textures but no lights</li>
<li>TEXLIGHT: render geometry with textures and lights</li>
</ul>

<p>
Together with the POINT and LINE shaders mentioned earlier, we end up having 6 different types of shaders in Processing.
</p>

<p>
Processing needs to use the right type of shader when drawing a piece of geometry, and by default it picks the correct shader by checking whether the geometry about to be rendered has lights or textures associated to it and then enabling the appropriate shader. When we set our own custom shader, we need to make sure that it belongs to the correct type to handle the subsequent drawing calls. For example, if simpleShader below is of type COLOR, then the following code will run without problems:
</p>

<pre>
<span style="color: #006699;">shader</span>(simpleShader);
<span style="color: #006699;">noLights</span>();
<span style="color: #006699;">fill</span>(180);
<span style="color: #006699;">rect</span>(0, 0, 100, 100);
</pre>

<p>However, if we try to do:</p>

<pre>
<span style="color: #006699;">shader</span>(simpleShader);
<span style="color: #006699;">lights</span>();
<span style="color: #006699;">fill</span>(180);
<span style="color: #006699;">rect</span>(0, 0, 100, 100);
</pre>

<p>
then Processing will print the warning <em>"Your shader cannot be used to render lit geometry, using default shader instead."</em>, but will still render the lit geometry by using its own default LIGHT shader instead the one provided in the <em>shader()</em> call.
</p>

<p>
The type of a shader must be specified inside the GLSL code, either in the vertex shader or the fragment shader (or both), by using a #define line, which are the following:
</p>

<ul>
<li>#define PROCESSING_POINT_SHADER for POINT shaders</li>
<li>#define PROCESSING_LINE_SHADER for LINE shaders</li>
<li>#define PROCESSING_COLOR_SHADER for COLOR shaders</li>
<li>#define PROCESSING_LIGHT_SHADER for LIGHT shaders</li>
<li>#define PROCESSING_TEXTURE_SHADER for TEXTURE shaders</li>
<li>#define PROCESSING_TEXLIGHT_SHADER for TEXLIGHT shaders</li>
</ul>

<p>In the next sections we will describe each type of shader in detail.</p>

<h3>4. Color shaders</h3>

<p>
We will go through the different types of shaders available in Processing and examine examples for each one. For the color, light, texture and texlight shaders we will use the same base geometry, a cylinder, throughout all the examples. The next code will serve as the basis for all the subsequent sketches:
</p>

<table>
<caption><b>Code listing 4.1:</b> Base cylinder.</caption>
<tr><td>
<pre>
<span style="color: #CC6633;">PShape</span> can;
<span style="color: #CC6633;">float</span> angle;
<span style="color: #CC6633;">PShader</span> colorShader;

<span style="color: #996633;">void</span> <span style="color: #006699;"><b>setup</b></span>() {
&nbsp;&nbsp;<span style="color: #006699;">size</span>(640, 360, <span style="color: #666666;">P3D</span>);
&nbsp;&nbsp;can&nbsp;=&nbsp;createCan(100,&nbsp;200,&nbsp;32);
}

<span style="color: #996633;">void</span> <span style="color: #006699;"><b>draw</b></span>() {    
&nbsp;&nbsp;<span style="color: #006699;">background</span>(0);  
&nbsp;&nbsp;<span style="color: #006699;">translate</span>(<span style="color: #FF6699;">width</span>/2, <span style="color: #FF6699;">height</span>/2);
&nbsp;&nbsp;<span style="color: #006699;">rotateY</span>(angle);  
&nbsp;&nbsp;<span style="color: #006699;">shape</span>(can);  
&nbsp;&nbsp;angle&nbsp;+=&nbsp;0.01;
}

<span style="color: #CC6633;">PShape</span> createCan(<span style="color: #CC6633;">float</span> r, <span style="color: #CC6633;">float</span> h, <span style="color: #CC6633;">int</span> detail) {
&nbsp;&nbsp;<span style="color: #006699;">textureMode</span>(<span style="color: #666666;">NORMAL</span>);
&nbsp;&nbsp;<span style="color: #CC6633;">PShape</span> sh = <span style="color: #006699;">createShape</span>();
&nbsp;&nbsp;sh.<span style="color: #006699;">beginShape</span>(<span style="color: #666666;">QUAD_STRIP</span>);
&nbsp;&nbsp;sh.<span style="color: #006699;">noStroke</span>();
&nbsp;&nbsp;<span style="color: #669933;">for</span> (<span style="color: #CC6633;">int</span> i = 0; i &lt;= detail; i++) {
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #CC6633;">float</span> angle = <span style="color: #666666;">TWO_PI</span> / detail;
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #CC6633;">float</span> x = <span style="color: #006699;">sin</span>(i * angle);
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #CC6633;">float</span> z = <span style="color: #006699;">cos</span>(i * angle);
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #CC6633;">float</span> u = <span style="color: #006699;">float</span>(i) / detail;
&nbsp;&nbsp;&nbsp;&nbsp;sh.<span style="color: #006699;">normal</span>(x, 0, z);
&nbsp;&nbsp;&nbsp;&nbsp;sh.<span style="color: #006699;">vertex</span>(x * r, -h/2, z * r, u, 0);
&nbsp;&nbsp;&nbsp;&nbsp;sh.<span style="color: #006699;">vertex</span>(x * r, +h/2, z * r, u, 1);    
&nbsp;&nbsp;}
&nbsp;&nbsp;sh.<span style="color: #006699;">endShape</span>(); 
&nbsp;&nbsp;<span style="color: #996633;">return</span> sh;
}
</pre>
</tr></td>
</table>

<p>
The output should be a just flat-colored white cylinder rotating around the Y axis. So let's write our first shader to render this cylinder in more interesting ways!
</p>

<p>
First, save the code in listing 1 and add the following two files in the data folder:
</p>

<table>
<caption><b>Code listing 4.2:</b> Vertex and fragment shaders for color rendering.</caption>
<tr><td>
<pre>
<b>colorvert.glsl:</b>

#define PROCESSING_COLOR_SHADER

uniform mat4 transform;

attribute vec4 vertex;
attribute vec4 color;

varying vec4 vertColor;

void main() {
  gl_Position = transform * vertex;    
  vertColor = color;
}

<b>colorfrag.glsl:</b>

#ifdef GL_ES
precision mediump float;
precision mediump int;
#endif

varying vec4 vertColor;

void main() {
  gl_FragColor = vertColor;
}
</pre>
</tr></td>
</table>

<p>
The type of shader is indicated here with the define in the vertex shader, but it could also be in the fragment shader. In this example, there is only one uniform variable, <em>transform,</em> which is a 4x4 matrix holding the product of the projection and the modelview matrices. The multiplication of a vertex in world coordinates by the <em>transform</em> matrix gives the <a href="http://www.songho.ca/opengl/gl_transform.html">clip coordinates</a>. The <em>vertex</em> and <em>color</em> attributes hold the position and color of the input vertex, respectively. The input color is copied without any modification to the <em>vertColor</em> varying, which passes the value down to the fragment shader. The fragment shader is nothing more than a pass-through shader since the color is sent to the output without any further modifications. The uniform <em>transform</em> and the attributes <em>vertex</em> and <em>color</em> are automatically set by Processing.
</p>

<p>
We can do some simple color manipulation by doing the following change in the fragment shader:
<p>

<pre>gl_FragColor = vec4(vec3(1) - vertColor.xyz, 1);</pre>

<p>
This will show on the screen the invert of the input color. So for instance, if we add <em>can.fill(255, 255, 0)</em> right after <em>can.beginShape(QUAD_STRIP)></em> then the cylinder should be painted blue.
</p>

<h3>5. Texture shaders</h3>

<p>
Rendering textured geometry requires additional uniforms and attributes in the shader. Let's look at the following sketch together with the accompanying fragment and vertex shaders:
</p>

<table>
<caption><b>Code listing 5.1:</b> Sketch for textured rendering (no lights).</caption>
<tr><td>
<pre>
<span style="color: #CC6633;">PImage</span> label;
<span style="color: #CC6633;">PShape</span> can;
<span style="color: #CC6633;">float</span> angle;

<span style="color: #CC6633;">PShader</span> texShader;

<span style="color: #996633;">void</span> <span style="color: #006699;"><b>setup</b></span>() {
&nbsp;&nbsp;<span style="color: #006699;">size</span>(640, 360, <span style="color: #666666;">P3D</span>);  
&nbsp;&nbsp;label&nbsp;=&nbsp;<span style="color: #006699;">loadImage</span>(<span style="color: #7D4793;">&quot;lachoy.jpg&quot;</span>);
&nbsp;&nbsp;can&nbsp;=&nbsp;createCan(100,&nbsp;200,&nbsp;32,&nbsp;label);
&nbsp;&nbsp;texShader&nbsp;=&nbsp;<span style="color: #006699;">loadShader</span>(<span style="color: #7D4793;">&quot;texfrag.glsl&quot;</span>, <span style="color: #7D4793;">&quot;texvert.glsl&quot;</span>);
}

<span style="color: #996633;">void</span> <span style="color: #006699;"><b>draw</b></span>() {    
&nbsp;&nbsp;<span style="color: #006699;">background</span>(0);
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;<span style="color: #006699;">shader</span>(texShader);  
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;<span style="color: #006699;">translate</span>(<span style="color: #FF6699;">width</span>/2, <span style="color: #FF6699;">height</span>/2);
&nbsp;&nbsp;<span style="color: #006699;">rotateY</span>(angle);  
&nbsp;&nbsp;<span style="color: #006699;">shape</span>(can);  
&nbsp;&nbsp;angle&nbsp;+=&nbsp;0.01;
}

<span style="color: #CC6633;">PShape</span> createCan(<span style="color: #CC6633;">float</span> r, <span style="color: #CC6633;">float</span> h, <span style="color: #CC6633;">int</span> detail, <span style="color: #CC6633;">PImage</span> tex) {
&nbsp;&nbsp;<span style="color: #006699;">textureMode</span>(<span style="color: #666666;">NORMAL</span>);
&nbsp;&nbsp;<span style="color: #CC6633;">PShape</span> sh = <span style="color: #006699;">createShape</span>();
&nbsp;&nbsp;sh.<span style="color: #006699;">beginShape</span>(<span style="color: #666666;">QUAD_STRIP</span>);
&nbsp;&nbsp;sh.<span style="color: #006699;">noStroke</span>();
&nbsp;&nbsp;sh.<span style="color: #006699;">texture</span>(tex);
&nbsp;&nbsp;<span style="color: #669933;">for</span> (<span style="color: #CC6633;">int</span> i = 0; i &lt;= detail; i++) {
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #CC6633;">float</span> angle = <span style="color: #666666;">TWO_PI</span> / detail;
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #CC6633;">float</span> x = <span style="color: #006699;">sin</span>(i * angle);
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #CC6633;">float</span> z = <span style="color: #006699;">cos</span>(i * angle);
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #CC6633;">float</span> u = <span style="color: #006699;">float</span>(i) / detail;
&nbsp;&nbsp;&nbsp;&nbsp;sh.<span style="color: #006699;">normal</span>(x, 0, z);
&nbsp;&nbsp;&nbsp;&nbsp;sh.<span style="color: #006699;">vertex</span>(x * r, -h/2, z * r, u, 0);
&nbsp;&nbsp;&nbsp;&nbsp;sh.<span style="color: #006699;">vertex</span>(x * r, +h/2, z * r, u, 1);    
&nbsp;&nbsp;}
&nbsp;&nbsp;sh.<span style="color: #006699;">endShape</span>(); 
&nbsp;&nbsp;<span style="color: #996633;">return</span> sh;
}

<b>texvert.glsl:</b>

#define PROCESSING_TEXTURE_SHADER

uniform mat4 transform;
uniform mat4 texMatrix;

attribute vec4 vertex;
attribute vec4 color;
attribute vec2 texCoord;

varying vec4 vertColor;
varying vec4 vertTexCoord;

void main() {
  gl_Position = transform * vertex;
    
  vertColor = color;
  vertTexCoord = texMatrix * vec4(texCoord, 1.0, 1.0);
}

<b>texfrag.glsl:</b>

#ifdef GL_ES
precision mediump float;
precision mediump int;
#endif

uniform sampler2D texture;

varying vec4 vertColor;
varying vec4 vertTexCoord;

void main() {
  gl_FragColor = texture2D(texture, vertTexCoord.st) * vertColor;
}
</pre>
</tr></td>
</table>

<p>
There is a new uniform in the vertex shader called <em>texMatrix</em>, which rescales the texture coordinates for each vertex (passed in the additional attribute <em>texCoord</em>), to take into account texture inversion along the Y-axis (as Processing's vertical axis is inverted with respect to OpenGL's), and non-power-of-two textures (a texture coordinate value of 1.0 for a npot texture will be rescaled to a smaller value that covers the npot range). In the fragment shader, we have a new uniform variable of type sampler2D, <em>texture</em>, which basically represents a pointer to the texture data,with the added capacity of sampling it at arbitrary texture coordinates (i.e: not necessarily at the center of a texel). Depending on the sampling configuration of the texture (linear, bilinear, etc.), a different interpolation algorithm will be used by the GPU. The result of this sketch is shown in the next figure:
</p>

<p><img src="imgs/texture.png"></p>

<p>
Implementing a pixelate effect becomes very easy at the level of the fragment shader. All we need to do is to modify the texture coordinate values, <em>vertTexCoord.st</em>, so that they are binned within a given number of cells, in this case 50:
</p>

<pre>
void main() {
  int si = int(vertTexCoord.s * 50.0);
  int sj = int(vertTexCoord.t * 50.0);  
  gl_FragColor = texture2D(texture, vec2(float(si) / 50.0, float(sj) / 50.0)) * vertColor;
}
</pre>

<p><img src="imgs/bintex.png"></p>

<p>
A final note about texture shaders is that they are used to render text as well. The reason for this is that the P2D/P3D renderers in Processing draw text as textured quads, which are handled by the texture shaders.
</p>




	<p>&nbsp;</p>

	<p><em>This tutorial is for Processing version 2.0+. If you see any errors or have comments, please <a href="https://github.com/processing/processing-web/issues?state=open">let us know</a>.</em></p>

</td>
	</tr>
  </table>
</p>